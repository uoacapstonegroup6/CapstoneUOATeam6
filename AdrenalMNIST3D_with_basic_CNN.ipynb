{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uoacapstonegroup6/CapstoneUOATeam6/blob/main/AdrenalMNIST3D_with_basic_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psI66xIdZ94u",
        "outputId": "81c33ec4-3d09-46e8-86ea-333f6a542d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting medmnist\n",
            "  Downloading medmnist-3.0.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from medmnist) (1.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.23.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from medmnist) (4.66.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from medmnist) (9.4.0)\n",
            "Collecting fire (from medmnist)\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from medmnist) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from medmnist) (0.19.0+cu121)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->medmnist) (2.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->medmnist) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (3.3)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (2024.8.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->medmnist) (0.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->medmnist) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->medmnist) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->medmnist) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->medmnist) (1.3.0)\n",
            "Downloading medmnist-3.0.2-py3-none-any.whl (25 kB)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117030 sha256=46015adc8f6a6dbf327265d135c394dbe9950d817be55ee1e10001a58c009f1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, medmnist\n",
            "Successfully installed fire-0.6.0 medmnist-3.0.2\n"
          ]
        }
      ],
      "source": [
        "# Install if required\n",
        "!pip install medmnist"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary packages\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import medmnist\n",
        "from medmnist import INFO, Evaluator"
      ],
      "metadata": {
        "id": "0sw7dd-uaKR2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating dataflags\n",
        "\n",
        "data_flag = 'adrenalmnist3d'\n",
        "download = True\n",
        "info = INFO[data_flag]\n",
        "task = info['task']\n",
        "n_channels = info['n_channels']\n",
        "n_classes = len(info['label'])\n",
        "\n",
        "DataClass = getattr(medmnist, info['python_class'])\n",
        "\n",
        "# load the data\n",
        "train_dataset = DataClass(split='train', download=download, size=64)\n",
        "test_dataset = DataClass(split='test', download=download, size=64)\n",
        "val_dataset = DataClass(split='val', download=download, size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDBSp-NRaL3j",
        "outputId": "0047639c-aad4-4287-8101-d240308b1fb6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://zenodo.org/records/10519652/files/adrenalmnist3d_64.npz?download=1 to /root/.medmnist/adrenalmnist3d_64.npz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1880591/1880591 [00:00<00:00, 1896071.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using downloaded and verified file: /root/.medmnist/adrenalmnist3d_64.npz\n",
            "Using downloaded and verified file: /root/.medmnist/adrenalmnist3d_64.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a function which converts a 3D image into 2D image in tensor format\n",
        "def image_3d_to_2d(dataset):\n",
        "\n",
        "  #Initializing empty lists to store 2D images\n",
        "  images = []\n",
        "  labels = []\n",
        "  index = []\n",
        "  print(\"Initial 3D images:\",len(dataset))\n",
        "\n",
        "  for i in range(len(dataset)):           #This loop runs for a few iterations as many as no. of 3D images\n",
        "    x, y = train_dataset[i]               #Extracting the image tensor (x) and label (y) from one 3D image\n",
        "    for j in range(len(x[0])):            #Getting into first batch of the selected 3D image. There will be only 1 batch and hence x[0] and len(x[0]) gives number of slices\n",
        "      img = x[0][j]                       #Array for every slice (j) is captured\n",
        "      arr = (img).astype(np.float32)      #Converting the normalized array into float\n",
        "      tensor = torch.tensor(arr)          #Converting array into tensor\n",
        "      images.append(tensor)               #Appending the image tensor into a list\n",
        "      labels.append(y[0])                 #Appending the label for the image into a list\n",
        "      index.append(i)                     #Appending the 3D image index into a list\n",
        "  print(\"Sliced 2D images:\",len(images))\n",
        "  return images, labels, index"
      ],
      "metadata": {
        "id": "hZtGnWmIaOl_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running the above function for train, test and validation.\n",
        "#Since the size=28, every 3D image has 28 slices\n",
        "\n",
        "train_2d = image_3d_to_2d(train_dataset)\n",
        "test_2d = image_3d_to_2d(test_dataset)\n",
        "val_2d = image_3d_to_2d(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4v0Jhw9ahcd",
        "outputId": "70ff3b0c-182e-4b91-e268-958ea281d9e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial 3D images: 1188\n",
            "Sliced 2D images: 76032\n",
            "Initial 3D images: 298\n",
            "Sliced 2D images: 19072\n",
            "Initial 3D images: 98\n",
            "Sliced 2D images: 6272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Once sliced up, images are saved as .pth file\n",
        "\n",
        "images, labels, index = train_2d\n",
        "torch.save({'images': images, 'labels': labels, 'index':index}, 'fracture_train_2d.pth')\n",
        "images, labels, index = test_2d\n",
        "torch.save({'images': images, 'labels': labels, 'index':index}, 'fracture_test_2d.pth')\n",
        "images, labels, index = val_2d\n",
        "torch.save({'images': images, 'labels': labels, 'index':index}, 'fracture_val_2d.pth')"
      ],
      "metadata": {
        "id": "Qq-_q1f7ajIS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved dataset\n",
        "data_train = torch.load('fracture_train_2d.pth')\n",
        "data_test = torch.load('fracture_test_2d.pth')\n",
        "data_val = torch.load('fracture_val_2d.pth')\n",
        "\n",
        "# Convert the loaded images and labels to tensors\n",
        "tr_images = torch.stack(data_train['images'])  # Stack list of tensors into a single tensor\n",
        "tr_images = tr_images.unsqueeze(1)              #Forcefully adding n_channel=1\n",
        "tr_images_rgb = tr_images.repeat(1, 3, 1, 1)\n",
        "tr_labels = torch.tensor(data_train['labels'])  # Convert list of labels into a tensor\n",
        "# if tr_labels.ndimension() == 1:  # If it's (num_samples,)\n",
        "    # num_classes = 3  # Replace with actual number of classes\n",
        "    # tr_labels = torch.nn.functional.one_hot(tr_labels, num_classes=num_classes).float()\n",
        "tr_index = torch.tensor(data_train['index'])  # Convert list of indices into a tensor\n",
        "\n",
        "ts_images = torch.stack(data_test['images'])  # Stack list of tensors into a single tensor\n",
        "ts_images = ts_images.unsqueeze(1)              #Forcefully adding n_channel=1\n",
        "ts_images_rgb = ts_images.repeat(1, 3, 1, 1)\n",
        "ts_labels = torch.tensor(data_test['labels'])  # Convert list of labels into a tensor\n",
        "# if ts_labels.ndimension() == 1:  # If it's (num_samples,)\n",
        "#     num_classes = 3  # Replace with actual number of classes\n",
        "#     ts_labels = torch.nn.functional.one_hot(ts_labels, num_classes=num_classes).float()\n",
        "ts_index = torch.tensor(data_test['index'])  # Convert list of indices into a tensor\n",
        "\n",
        "val_images = torch.stack(data_val['images'])  # Stack list of tensors into a single tensor\n",
        "val_images = val_images.unsqueeze(1)              #Forcefully adding n_channel=1\n",
        "val_images_rgb = val_images.repeat(1, 3, 1, 1)\n",
        "val_labels = torch.tensor(data_val['labels'])  # Convert list of labels into a tensor\n",
        "val_index = torch.tensor(data_val['index'])  # Convert list of indices into a tensor\n",
        "\n",
        "# Create a TensorDataset from the loaded data\n",
        "dataset_train = data.TensorDataset(tr_images_rgb, tr_labels, tr_index)\n",
        "dataset_test = data.TensorDataset(ts_images_rgb, ts_labels, ts_index)\n",
        "dataset_val = data.TensorDataset(val_images_rgb, val_labels, val_index)\n",
        "\n",
        "# Create a DataLoader for batching and shuffling\n",
        "train_loader = data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
        "test_loader = data.DataLoader(dataset_test, batch_size=128, shuffle=True)\n",
        "val_loader = data.DataLoader(dataset_val, batch_size=128, shuffle=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng01fonkak2M",
        "outputId": "a07ae2c6-d2ff-473d-97f0-14154f98025b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-52d39c6dc2ed>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_train = torch.load('fracture_train_2d.pth')\n",
            "<ipython-input-8-52d39c6dc2ed>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_test = torch.load('fracture_test_2d.pth')\n",
            "<ipython-input-8-52d39c6dc2ed>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_val = torch.load('fracture_val_2d.pth')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To see each batch of images\n",
        "# for batch in test_loader:\n",
        "#     batch_images, batch_labels, batch_index = batch\n",
        "#     print(batch_images.shape)  # Should be (batch_size, 1, 28, 28)\n",
        "#     print(batch_labels)"
      ],
      "metadata": {
        "id": "0Y78eahkqkit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9zDwpOheN_u",
        "outputId": "ed20fac4-6a1a-46ba-d512-f0cdeaf2f846"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Sep 18 06:12:33 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Omid-Nejati/MedViT.git\n",
        "%cd /content/MedViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koczSMhmeRku",
        "outputId": "36efde2f-b1bc-4e81-aa02-d8cb9005851c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MedViT'...\n",
            "remote: Enumerating objects: 176, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 176 (delta 87), reused 134 (delta 61), pack-reused 1 (from 1)\u001b[K\n",
            "Receiving objects: 100% (176/176), 820.48 KiB | 3.65 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n",
            "/content/MedViT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9ucnZ8TWyh0",
        "outputId": "4583b08c-678e-42bf-a843-0c28c01dcbc4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (0.8.0)\n",
            "Collecting timm (from -r requirements.txt (line 2))\n",
            "  Downloading timm-1.0.9-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: medmnist in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (0.23.2)\n",
            "Collecting fvcore (from -r requirements.txt (line 8))\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (4.66.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (9.4.0)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (0.6.0)\n",
            "Collecting torchattacks (from -r requirements.txt (line 12))\n",
            "  Downloading torchattacks-3.5.1-py3-none-any.whl.metadata (927 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.19.0+cu121)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 2)) (0.24.7)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->-r requirements.txt (line 2)) (0.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 5)) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 6)) (3.5.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (3.3)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2.34.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (2024.8.30)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->-r requirements.txt (line 7)) (0.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore->-r requirements.txt (line 8))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 8)) (2.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore->-r requirements.txt (line 8)) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore->-r requirements.txt (line 8))\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 11)) (1.16.0)\n",
            "Collecting requests~=2.25.1 (from torchattacks->-r requirements.txt (line 12))\n",
            "  Downloading requests-2.25.1-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 13)) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 13)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 13)) (1.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 13)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 13)) (2024.6.1)\n",
            "Collecting portalocker (from iopath>=0.1.7->fvcore->-r requirements.txt (line 8))\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting chardet<5,>=3.0.2 (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12))\n",
            "  Downloading chardet-4.0.0-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting idna<3,>=2.5 (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12))\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12))\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.25.1->torchattacks->-r requirements.txt (line 12)) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 13)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 13)) (1.3.0)\n",
            "Downloading timm-1.0.9-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchattacks-3.5.1-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.0/142.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: fvcore, iopath\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61395 sha256=0c1834770b95a1f3ac1fa3a2b4afaffeb9dbf095a9c05d15dcd79990671d4131\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31529 sha256=1858822d44bbf7f6c53410716ab2f7d09c8b2b3efd2c561e9afc2ed2ccd37f15\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "Successfully built fvcore iopath\n",
            "Installing collected packages: yacs, urllib3, portalocker, idna, chardet, requests, iopath, fvcore, torchattacks, timm\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.8\n",
            "    Uninstalling idna-3.8:\n",
            "      Successfully uninstalled idna-3.8\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.17.0 requires requests>=2.27.1, but you have requests 2.25.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.25.1 which is incompatible.\n",
            "tweepy 4.14.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\n",
            "yfinance 0.2.43 requires requests>=2.31, but you have requests 2.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chardet-4.0.0 fvcore-0.1.5.post20221221 idna-2.10 iopath-0.1.10 portalocker-2.10.1 requests-2.25.1 timm-1.0.9 torchattacks-3.5.1 urllib3-1.26.20 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from MedViT import MedViT_small as tiny, MedViT_base, MedViT_large\n",
        "\n",
        "model = tiny()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gcycgIVeNqF",
        "outputId": "f663fedf-f293-4164-d24f-ce89418e3f23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "initialize_weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you're using a GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "9MrreOh6kMRg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.proj_head[0] = torch.nn.Linear(in_features=1024, out_features=3, bias=True)"
      ],
      "metadata": {
        "id": "qIuDPwzR4tTL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "XgoMb5GM-vas"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluation\n",
        "\n",
        "# task = 'multi-label, binary-class'\n",
        "model.eval()\n",
        "y_true = torch.tensor([]).cuda()\n",
        "y_score = torch.tensor([]).cuda()\n",
        "\n",
        "data_loader = test_loader\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets, index in data_loader:\n",
        "        inputs, targets, index = inputs.cuda(), targets.cuda(), index.cuda()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        if task == 'multi-label, binary-class':\n",
        "            targets = targets.to(torch.float32)\n",
        "            outputs = outputs.softmax(dim=-1)\n",
        "            # print(1)\n",
        "        else:\n",
        "            targets = targets.squeeze().long()\n",
        "            outputs = outputs.softmax(dim=-1)\n",
        "            targets = targets.float().resize_(len(targets), 1)\n",
        "            # print(0)\n",
        "\n",
        "        y_true = torch.cat((y_true, targets), 0)\n",
        "        y_score = torch.cat((y_score, outputs), 0)\n",
        "\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_score = y_score.detach().cpu().numpy()"
      ],
      "metadata": {
        "id": "OEFIqvJTeNcV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "actuals = y_true.squeeze().tolist()\n",
        "predictions = []\n",
        "y_score = y_score.tolist()\n",
        "for i in range(len(y_score)):\n",
        "  predictions.append(y_score[i].index(max(y_score[i])))"
      ],
      "metadata": {
        "id": "y5Y-0UaJBjJT"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct_predictions = sum(a==p for a, p in zip(actuals, predictions))\n",
        "accuracy = (correct_predictions / len(actuals)) * 100"
      ],
      "metadata": {
        "id": "TO5UhasyCJZe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy"
      ],
      "metadata": {
        "id": "qTQnt7FsXUuk",
        "outputId": "4ae7010e-9dc2-4cb9-aec1-d83a69f654b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.825503355704697"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(actuals, predictions, average='macro')\n",
        "recall = recall_score(actuals, predictions, average='macro')\n",
        "f1 = f1_score(actuals, predictions, average='macro')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COJ9i0dGW8Ow",
        "outputId": "887323c0-7ac1-4d55-9ed4-505443837b75"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "MbZ7y0erEDzl",
        "outputId": "3a4ce396-d562-4a7d-c032-070ec6b0104f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 23.825503355704697\n",
            "Precision: 0.11912751677852348\n",
            "Recall: 0.5\n",
            "F1 Score: 0.19241192411924118\n"
          ]
        }
      ]
    }
  ]
}